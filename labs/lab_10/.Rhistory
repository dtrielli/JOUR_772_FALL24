knitr::opts_chunk$set(echo = TRUE)
# Turn off scientific notation
options(scipen=999)
# Load the tidyverse.
library(tidyverse)
library(tidycensus)
library(janitor)
library(tigris)
#Taking county geometry from tigris
counties <- counties()
knitr::opts_chunk$set(echo = TRUE)
# Turn off scientific notation
options(scipen=999)
# Load the tidyverse.
library(tidyverse)
library(tidycensus)
library(janitor)
library(tigris)
#Calculating percentages for both candidates
md_statewide <- read_csv("data/md_statewide_county.csv") |>
clean_names() |>
mutate(geoid = as.character(geoid)) |>
mutate(total_ag = peroutka + brown + ag_writein) |>
mutate(total_gov = cox + moore + lashar + wallace + harding + sefcik + gov_writein) |>
mutate(pct_brown = brown/total_ag * 100, pct_moore = moore/total_gov *100, moore_brown_diff = pct_moore - pct_brown)
#Taking county geometry from tigris
counties <- counties()
#Filtering out MD counties
md_counties <- counties |>
filter(STATEFP == "24")
#Merging geometries with voting data
md_statewide_geometry <- md_counties |>
inner_join(md_statewide, by=c('GEOID'='geoid'))
#Mapping it out
ggplot() +
geom_sf(data=md_statewide_geometry, aes(fill=moore_brown_diff)) +
theme_minimal() +
scale_fill_viridis_b(option="magma")
#Read data and calculating rate of outages per customer
md_county_outages <- read_csv("data/county_outages_march2023.csv") |> mutate(rate = outage_customers/total_customers*10000)
# Fixing
md_counties_fixed <- md_counties |> mutate(NAME = case_when(
NAMELSAD == 'Baltimore city' ~ 'Baltimore City',
.default = NAME
))
#Merge geometries with outage data
md_county_outages_geo <- md_counties_fixed |> inner_join(md_county_outages, by=c('NAME'='county'))
#Map it out
ggplot() +
geom_sf(data=md_county_outages_geo, aes(fill=rate)) +
theme_minimal() +
scale_fill_viridis_b(option="magma",trans = "log")
electric_hybrid <- read_csv("data/ev_2020_2023.csv") |> mutate(pct_change = (march_2023-july_2020)/july_2020 *100)
electric_hybrid_with_geo <- md_counties_fixed |> inner_join(electric_hybrid, by=c('NAME'='county'))
ggplot() +
geom_sf(data=electric_hybrid_with_geo, aes(fill=pct_change)) +
theme_minimal() +
scale_fill_viridis_b(option="magma", trans='log')
md_county_population <- get_acs(geography = "county",
variables = c(population = "B01001_001"),
year = 2021,
state = "MD")
electric_hybrid_with_geo_pop <- electric_hybrid_with_geo |> inner_join(md_county_population, by=('GEOID')) |> mutate(rate = march_2023/estimate*10000)
ggplot() +
geom_sf(data=electric_hybrid_with_geo_pop, aes(fill=rate)) +
theme_minimal() +
scale_fill_viridis_b(option="magma", trans='log')
md_statewide <- read_csv("data/md_statewide_county.csv",
col_types = cols(
Cox = col_double(),
Moore = col_double()
)) |> clean_names()
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidytext")
library(rvest)
library(tidyverse)
library(tidytext)
library(janitor)
library(lubridate)
releases <- read_rds("data/cardin_releases.rds")
urls <- releases |> top_n(10) |> pull(url)
release_text <- tibble(url = character(), text = character())
# loop over each url in the list of urls
for (u in urls){
# wait a fraction of a second so we don't hammer the server
Sys.sleep(0.2)
# read in the html from the url
html <- u |> read_html()
# use the xpath of the text of the release to grab it and call html_text() on it
text <- html |>
html_element(xpath="/html/body/div/div/div/div/div/div/div[2]/div[1]/div/div[4]") |>
html_text()
release_text <- release_text |> add_row(url = u, text = str_squish(text))
}
release_text
releases <- releases |>
mutate(text = gsub("http.*","", text))
a_list_of_words <- c("Dog", "dog", "dog", "cat", "cat", ",")
unique(a_list_of_words)
unique_words <- releases |> select(text) |>
unnest_tokens(word, text)
View(unique_words)
unique_words |>
count(word, sort = TRUE) |>
top_n(25) |>
mutate(word = reorder(word, n)) |>
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in Cardin releases")
data("stop_words")
stop_words <- stop_words |>
add_row(word = "ben") |>
add_row(word = "cardin") |>
add_row(word = "senator") |>
add_row(word = "senators") |>
add_row(word = "maryland") |>
add_row(word = 'federal') |>
add_row(word = 'u.s') |>
add_row(word = 'md') |>
add_row(word = 'senate') |>
add_row(word = "hollen") |>
add_row(word = "van") |>
add_row(word = "chris") |>
add_row(word = "project") |>
add_row(word = "program")
unique_words |>
anti_join(stop_words) |>
group_by(word) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
unique_words_2022 <- releases |>
filter(year(date) == 2022) |>
select(text) |>
unnest_tokens(word, text)
unique_words_2024 <- releases |>
filter(year(date) == 2024) |>
select(text) |>
unnest_tokens(word, text)
unique_words_2022 |>
anti_join(stop_words) |>
group_by(word) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
unique_words_2024 |>
anti_join(stop_words) |>
group_by(word) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
releases |>
filter(year(date) == 2022) |>
unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
separate(bigram, c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word) |>
mutate(bigram = paste(word1, word2, sep=" ")) |>
group_by(bigram) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
releases |>
filter(year(date) == 2024) |>
unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
separate(bigram, c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word) |>
mutate(bigram = paste(word1, word2, sep=" ")) |>
group_by(bigram) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
bing <- get_sentiments("bing")
bing_word_counts_2022 <- unique_words_2022 |>
inner_join(bing) |>
count(word, sentiment, sort = TRUE)
bing_word_counts_2024 <- unique_words_2024 |>
inner_join(bing) |>
count(word, sentiment, sort = TRUE)
View(bing_word_counts_2022)
View(bing_word_counts_2024)
#Read data and calculating rate of outages per customer
md_county_outages <- read_csv("data/county_outages_march2023.csv") |> mutate(rate = outage_customers/total_customers*10000)
# Fixing
md_counties_fixed <- md_counties |> mutate(NAME = case_when(
NAMELSAD == 'Baltimore city' ~ 'Baltimore City',
.default = NAME
))
View(md_county_outages)
# Fixing
md_counties_fixed <- md_counties |> mutate(NAME = case_when(
NAMELSAD == 'Baltimore city' ~ 'Baltimore City',
.default = NAME
))
#Calculating percentages for both candidates
md_statewide <- read_csv("data/md_statewide_county.csv") |>
clean_names() |>
mutate(geoid = as.character(geoid)) |>
mutate(total_ag = peroutka + brown + ag_writein) |>
mutate(total_gov = cox + moore + lashar + wallace + harding + sefcik + gov_writein) |>
mutate(pct_brown = brown/total_ag * 100, pct_moore = moore/total_gov *100, moore_brown_diff = pct_moore - pct_brown)
#Taking county geometry from tigris
counties <- counties()
#Filtering out MD counties
md_counties <- counties |>
filter(STATEFP == "24")
#Merging geometries with voting data
md_statewide_geometry <- md_counties |>
inner_join(md_statewide, by=c('GEOID'='geoid'))
#Mapping it out
ggplot() +
geom_sf(data=md_statewide_geometry, aes(fill=moore_brown_diff)) +
theme_minimal() +
scale_fill_viridis_b(option="magma")
#Read data and calculating rate of outages per customer
md_county_outages <- read_csv("data/county_outages_march2023.csv") |> mutate(rate = outage_customers/total_customers*10000)
# Fixing
md_counties_fixed <- md_counties |> mutate(NAME = case_when(
NAMELSAD == 'Baltimore city' ~ 'Baltimore City',
.default = NAME
))
#Merge geometries with outage data
md_county_outages_geo <- md_counties_fixed |> inner_join(md_county_outages, by=c('NAME'='county'))
#Map it out
ggplot() +
geom_sf(data=md_county_outages_geo, aes(fill=rate)) +
theme_minimal() +
scale_fill_viridis_b(option="magma",trans = "log")
View(counties)
View(md_counties)
#Calculating percentages for both candidates
md_statewide <- read_csv("data/md_statewide_county.csv") |>
clean_names() |>
mutate(geoid = as.character(geoid)) |>
mutate(total_ag = peroutka + brown + ag_writein) |>
mutate(total_gov = cox + moore + lashar + wallace + harding + sefcik + gov_writein) |>
mutate(pct_brown = brown/total_ag * 100, pct_moore = moore/total_gov *100, moore_brown_diff = pct_moore - pct_brown)
#Taking county geometry from tigris
counties <- counties()
#Filtering out MD counties
md_counties <- counties |>
filter(STATEFP == "24")
#Merging geometries with voting data
md_statewide_geometry <- md_counties |>
inner_join(md_statewide, by=c('GEOID'='geoid'))
#Mapping it out
ggplot() +
geom_sf(data=md_statewide_geometry, aes(fill=moore_brown_diff)) +
theme_minimal() +
scale_fill_viridis_b(option="magma")
#Read data and calculating rate of outages per customer
md_county_outages <- read_csv("data/county_outages_march2023.csv") |> mutate(rate = outage_customers/total_customers*10000)
# Fixing
md_counties_fixed <- md_counties |> mutate(NAME = case_when(
NAMELSAD == 'Baltimore city' ~ 'Baltimore City',
.default = NAME
))
View(md_counties_fixed)
md_county_outages_geo <- md_counties_fixed |> inner_join(md_county_outages, by=c('NAME'='county'))
#Map it out
ggplot() +
geom_sf(data=md_county_outages_geo, aes(fill=rate)) +
theme_minimal() +
scale_fill_viridis_b(option="magma",trans = "log")
electric_hybrid <- read_csv("data/ev_2020_2023.csv") |> mutate(pct_change = (march_2023-july_2020)/july_2020 *100)
electric_hybrid_with_geo <- md_counties_fixed |> inner_join(electric_hybrid, by=c('NAME'='county'))
ggplot() +
geom_sf(data=electric_hybrid_with_geo, aes(fill=pct_change)) +
theme_minimal() +
scale_fill_viridis_b(option="magma", trans='log')
md_county_population <- get_acs(geography = "county",
variables = c(population = "B01001_001"),
year = 2021,
state = "MD")
electric_hybrid_with_geo_pop <- electric_hybrid_with_geo |> inner_join(md_county_population, by=('GEOID')) |> mutate(rate = march_2023/estimate*10000)
ggplot() +
geom_sf(data=electric_hybrid_with_geo_pop, aes(fill=rate)) +
theme_minimal() +
scale_fill_viridis_b(option="magma", trans='log')
knitr::opts_chunk$set(echo = TRUE)
#install.packages("tidytext")
library(rvest)
library(tidyverse)
library(tidytext)
library(janitor)
library(lubridate)
releases <- read_rds("data/cardin_releases.rds")
View(releases)
View(releases)
urls <- releases |> top_n(10) |> pull(url)
release_text <- tibble(url = character(), text = character())
# loop over each url in the list of urls
for (u in urls){
# wait a fraction of a second so we don't hammer the server
Sys.sleep(0.2)
# read in the html from the url
html <- u |> read_html()
# use the xpath of the text of the release to grab it and call html_text() on it
text <- html |>
html_element(xpath="/html/body/div/div/div/div/div/div/div[2]/div[1]/div/div[4]") |>
html_text()
release_text <- release_text |> add_row(url = u, text = str_squish(text))
}
release_text
View(releases)
releases <- releases |>
mutate(text = gsub("http.*","", text))
View(releases)
a_list_of_words <- c("Dog", "dog", "dog", "cat", "cat", ",")
a_list_of_words <- c("Dog", "dog", "dog", "cat", "cat", ",")
unique(a_list_of_words)
unique_words <- releases |> select(text) |>
unnest_tokens(word, text)
View(unique_words)
unique_words |>
count(word, sort = TRUE) |>
top_n(25) |>
mutate(word = reorder(word, n)) |>
ggplot(aes(x = word, y = n)) +
geom_col() +
xlab(NULL) +
coord_flip() +
labs(x = "Count",
y = "Unique words",
title = "Count of unique words found in Cardin releases")
data("stop_words")
data("stop_words")
stop_words <- stop_words |>
add_row(word = "ben") |>
add_row(word = "cardin") |>
add_row(word = "senator") |>
add_row(word = "senators") |>
add_row(word = "maryland") |>
add_row(word = 'federal') |>
add_row(word = 'u.s') |>
add_row(word = 'md') |>
add_row(word = 'senate') |>
add_row(word = "hollen") |>
add_row(word = "van") |>
add_row(word = "chris") |>
add_row(word = "project") |>
add_row(word = "program")
unique_words |>
anti_join(stop_words) |>
group_by(word) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
unique_words_2022 <- releases |>
filter(year(date) == 2022) |>
select(text) |>
unnest_tokens(word, text)
unique_words_2024 <- releases |>
filter(year(date) == 2024) |>
select(text) |>
unnest_tokens(word, text)
unique_words_2022 |>
anti_join(stop_words) |>
group_by(word) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
unique_words_2024 |>
anti_join(stop_words) |>
group_by(word) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
releases |>
filter(year(date) == 2022) |>
unnest_tokens(bigram, text, token = "ngrams", n = 2)
releases |>
filter(year(date) == 2022) |>
unnest_tokens(bigram, text, token = "ngrams", n = 2)
releases |>
filter(year(date) == 2022) |>
unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
separate(bigram, c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word) |>
mutate(bigram = paste(word1, word2, sep=" ")) |>
group_by(bigram) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
releases |>
filter(year(date) == 2024) |>
unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
separate(bigram, c("word1", "word2"), sep = " ") |>
filter(!word1 %in% stop_words$word) |>
filter(!word2 %in% stop_words$word) |>
mutate(bigram = paste(word1, word2, sep=" ")) |>
group_by(bigram) |>
tally(sort=TRUE) |>
mutate(percent = (n/sum(n))*100) |>
top_n(10)
bing <- get_sentiments("bing")
bing_word_counts_2022 <- unique_words_2022 |>
inner_join(bing) |>
count(word, sentiment, sort = TRUE)
bing_word_counts_2024 <- unique_words_2024 |>
inner_join(bing) |>
count(word, sentiment, sort = TRUE)
View(bing_word_counts_2022)
View(bing_word_counts_2024)
View(bing)
bing <- get_sentiments("bing")
bing_word_counts_2022 <- unique_words_2022 |>
inner_join(bing) |>
count(word, sentiment, sort = TRUE)
bing_word_counts_2024 <- unique_words_2024 |>
inner_join(bing) |>
count(word, sentiment, sort = TRUE)
View(bing_word_counts_2022)
View(bing_word_counts_2024)
